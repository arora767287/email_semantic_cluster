# Topic Modeling with Emails

## Background
Email filtering is an extremely repetitive task affecting lots of people that can be automated with AI. About 56.5 percent of emails sent in 2022 were spam and thus, email filtration systems can become extremely useful. Unfortunately, right now, the current email filtration systems still allow certain spam mail and unimportant mail to pass through the filtration system. This not only wastes time for the user, but also detracts attention away from actual important mail. Our goal in this project is to create an email clustering system that accurately clusters emails based on specific topics a user may want to see in their inbox.

## Problem Definition
Researchers, students, and professionals all use their emails as a central point of communication. However, because of how universally used emailing is, it often becomes subject to cluttering and a lack of organization with incoming messages of varying importance to receivers. This can lead to a backlog of important messages being obscured by less relevant ones, causing very urgent circumstances and concerns to be unresolved and both incoherence as well as reduced productivity for thousands of individuals—the scale of this problem and ubiquity of email is what makes these consequences even more alarming.

## Methods
In order to tackle this issue we plan to use 3 different models to get a survey of the best possible approach to this issue. One potential pipeline we are considering involves Term Frequency-Inverse Document Frequency (TF-IDF), Principal Component Analysis (PCA) and the Pachinko Allocation Model (PAM). TF-IDF is a method used to indicate the significance of certain phrases among a set of documents as inputs. This provides a measurement of relevance of every term against an entire body of input text. We plan to use this to help identify keywords and extract features from a corpus of emails we pass in as input data. Then, we plan to use PCA to reduce the dimensionality of the keywords we choose to analyze emails across that have been extracted from using TF-IDF and pick those that create the greatest variance in email categorization. Finally, the Pachinko Allocation Model, which is a topic modelling technique used for clustering, will be implemented using the most significant keywords as labels for eventual probabilistic classify of every email into a topic.

As an alternative, we are exploring the use of Doc2Vec in a pipeline with Unsupervised Text Clustering using Lbl2Vec for Topic Modeling. Doc2Vec is a technique that extends from the popular word2vec model and allows vector representations of entire bodies of texts and documents to be learned. We would use this to turn the emails in our dataset into fixed-size vector embedding representations that include their context and semantic meanings. The resulting vectorized representations of the emails will then be clustered using a kNN model, where the centroids of the clusters will be vector representations of semantic content, meaning and contexts. We will then use the Lbl2Vec model to extract a meaning from these vector embeddings by tying them to labels that can be used to meaningfully categorize the email clusters generated through the kNN clustering output.

Finally, the last approach we are exploring is using a pipeline consisting BERT and Non-Negative Matrix Clustering. BERT is a pre-trained neural network that has been trained on a very large body of textual data and represents words as they relate to surrounding text in both the left-to-right and right-to-left directions. After using this vector embedding for the text in the emails, we plan to convert the BERT vector embeddings into a Document Term Matrix where every row represents an email and every column represents a term (word). We wil most likely apply a dimensionality reducing technique like PCA to produce this DTM as well. Finally, we will apply the Non-Negative Matrix Clustering algorithm to generate a basis and coefficients matrix that help cluster the emails in the DTM into particular topics.

## Dataset
As we’ll need an extensive training data set of emails to facilitate the above methods, we plan to use the Enron dataset, which contains emails of about 150 users and has been compiled with 500,000 messages.

## Potenial Results and Discussion
As this is an unsupervised tasks the metrics we use will be especially important in order to get clear and coherent clusters that accurately model the topic. The metrics we plan to use are related to topic coherence. To give a brief intuition behind the metric, our general goal is to maxmize the similarity between documents/elements in a given cluster and maximize the difference of similarity between documents/elements of different clusters. This can be done quite easily using the topic coherence metrics based around UMass’ formulation and Word2Vec usage.

[Gantt and Contribution Chart](https://docs.google.com/spreadsheets/d/1ZUl8Xywp4VTTNtC-8Wq8ZxpYnzXYNJLe/edit?usp=sharing&ouid=101698207149759013919&rtpof=true&sd=true)
